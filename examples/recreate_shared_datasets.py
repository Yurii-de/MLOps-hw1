#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –ø–µ—Ä–µ—Å–æ–∑–¥–∞–Ω–∏—è —à–∞–±–ª–æ–Ω–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –∫–∞–∫ –æ–±—â–∏—Ö (owner=None).
–£–¥–∞–ª—è–µ—Ç —Å—Ç–∞—Ä—ã–µ –¥–∞—Ç–∞—Å–µ—Ç—ã iris –∏ adult –∏ —Å–æ–∑–¥–∞–µ—Ç –Ω–æ–≤—ã–µ –∫–∞–∫ –æ–±—â–∏–µ.
"""

import shutil
import sys
from pathlib import Path

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ src –≤ PYTHONPATH
BASE_DIR = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(BASE_DIR))

import pandas as pd

from src.models.dataset_storage import DatasetStorage


def main():
    """–ü–µ—Ä–µ—Å–æ–∑–¥–∞—Ç—å —à–∞–±–ª–æ–Ω–Ω—ã–µ –¥–∞—Ç–∞—Å–µ—Ç—ã –∫–∞–∫ –æ–±—â–∏–µ."""
    print("=" * 60)
    print("–ü–µ—Ä–µ—Å–æ–∑–¥–∞–Ω–∏–µ —à–∞–±–ª–æ–Ω–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –∫–∞–∫ –æ–±—â–∏—Ö")
    print("=" * 60)

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è storage
    storage_dir = BASE_DIR / "datasets"
    dataset_storage = DatasetStorage(storage_dir)

    # –î–∞—Ç–∞—Å–µ—Ç—ã –¥–ª—è –ø–µ—Ä–µ—Å–æ–∑–¥–∞–Ω–∏—è
    datasets_to_recreate = {
        "iris": {
            "file": "test_data/iris.csv",
            "target": "species",
            "description": "Iris classification dataset (–æ–±—â–∏–π)"
        },
        "adult": {
            "file": "test_data/adult.csv",
            "target": "income",
            "description": "Adult income classification dataset (–æ–±—â–∏–π)"
        }
    }

    for dataset_id, info in datasets_to_recreate.items():
        print(f"\nüîÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞: {dataset_id}")

        # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–π –¥–∞—Ç–∞—Å–µ—Ç –µ—Å–ª–∏ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
        old_dataset_path = storage_dir / f"{dataset_id}.pkl"
        old_encoders_dir = storage_dir / f"{dataset_id}_encoders"
        old_target_encoder = storage_dir / f"{dataset_id}_target_encoder.json"

        if old_dataset_path.exists():
            print(f"  ‚ùå –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–∞—Ä–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞: {old_dataset_path}")
            old_dataset_path.unlink()

        if old_encoders_dir.exists():
            print(f"  ‚ùå –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–∞—Ä—ã—Ö —ç–Ω–∫–æ–¥–µ—Ä–æ–≤: {old_encoders_dir}")
            shutil.rmtree(old_encoders_dir)

        if old_target_encoder.exists():
            print(f"  ‚ùå –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–∞—Ä–æ–≥–æ target —ç–Ω–∫–æ–¥–µ—Ä–∞: {old_target_encoder}")
            old_target_encoder.unlink()

        # –ó–∞–≥—Ä—É–∂–∞–µ–º CSV
        csv_path = BASE_DIR / info["file"]
        if not csv_path.exists():
            print(f"  ‚ö†Ô∏è –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {csv_path}, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
            continue

        print(f"  üìÇ –ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑: {csv_path}")
        df = pd.read_csv(csv_path)

        # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç –∫–∞–∫ –æ–±—â–∏–π (owner=None)
        print("  üíæ –°–æ–∑–¥–∞–Ω–∏–µ –æ–±—â–µ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞...")
        dataset_info = dataset_storage.save_dataset(
            dataset_id=dataset_id,
            df=df,
            target_column=info["target"],
            preprocess_categorical=True,
            owner=None  # –î–µ–ª–∞–µ–º –æ–±—â–∏–º!
        )

        print(f"  ‚úÖ –î–∞—Ç–∞—Å–µ—Ç '{dataset_id}' —Å–æ–∑–¥–∞–Ω –∫–∞–∫ –æ–±—â–∏–π")
        print(f"     –°—Ç—Ä–æ–∫: {dataset_info['rows']}")
        print(f"     –ö–æ–ª–æ–Ω–æ–∫: {dataset_info['columns']}")
        print(f"     Target: {dataset_info['target_column']}")
        print(f"     –í–ª–∞–¥–µ–ª–µ—Ü: {dataset_info.get('owner', '–û–±—â–∏–π')}")

        if dataset_info.get('categorical_columns_processed'):
            print(f"     –ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã: {len(dataset_info['categorical_columns_processed'])}")

    print("\n" + "=" * 60)
    print("‚úÖ –ü–µ—Ä–µ—Å–æ–∑–¥–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
    print("=" * 60)
    print("\n–¢–µ–ø–µ—Ä—å –¥–∞—Ç–∞—Å–µ—Ç—ã 'iris' –∏ 'adult' —è–≤–ª—è—é—Ç—Å—è –æ–±—â–∏–º–∏ –∏:")
    print("  üåê –î–æ—Å—Ç—É–ø–Ω—ã –≤—Å–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º")
    print("  üîí –ó–∞—â–∏—â–µ–Ω—ã –æ—Ç —É–¥–∞–ª–µ–Ω–∏—è")
    print("  üìä –û—Ç–æ–±—Ä–∞–∂–∞—é—Ç—Å—è —Å –º–µ—Ç–∫–æ–π '–í–ª–∞–¥–µ–ª–µ—Ü: –û–±—â–∏–π'")

if __name__ == "__main__":
    main()
